## This file provides starter code for extracting features from the xml files and
## for doing some learning.
##
## The basic set-up:
## ----------------
## main() will run code to extract features, learn, and make predictions.
##
## extract_feats() is called by main(), and it will iterate through the
## train/test directories and parse each xml file into an xml.etree.ElementTree,
## which is a standard python object used to represent an xml file in memory.
## (More information about xml.etree.ElementTree objects can be found here:
## http://docs.python.org/2/library/xml.etree.elementtree.html
## and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)
## It will then use a series of "feature-functions" that you will write/modify
## in order to extract dictionaries of features from each ElementTree object.
## Finally, it will produce an N x D sparse design matrix containing the union
## of the features contained in the dictionaries produced by your "feature-functions."
## This matrix can then be plugged into your learning algorithm.
##
## The learning and prediction parts of main() are largely left to you, though
## it does contain code that randomly picks class-specific weights and predicts
## the class with the weights that give the highest score. If your prediction
## algorithm involves class-specific weights, you should, of course, learn
## these class-specific weights in a more intelligent way.
##
## Feature-functions:
## --------------------
## "feature-functions" are functions that take an ElementTree object representing
## an xml file (which contains, among other things, the sequence of system calls a
## piece of potential malware has made), and returns a dictionary mapping feature names to
## their respective numeric values.
## For instance, a simple feature-function might map a system call history to the
## dictionary {'first_call-load_image': 1}. This is a boolean feature indicating
## whether the first system call made by the executable was 'load_image'.
## Real-valued or count-based features can of course also be defined in this way.
## Because this feature-function will be run over ElementTree objects for each
## software execution history instance, we will have the (different)
## feature values of this feature for each history, and these values will make up
## one of the columns in our final design matrix.
## Of course, multiple features can be defined within a single dictionary, and in
## the end all the dictionaries returned by feature functions (for a particular
## training example) will be unioned, so we can collect all the feature values
## associated with that particular instance.
##
## Two example feature-functions, first_last_system_call_feats() and
## system_call_count_feats(), are defined below.
## The first of these functions indicates what the first and last system-calls
## made by an executable are, and the second records the total number of system
## calls made by an executable.
##
## What you need to do:
## --------------------
## 1. Write new feature-functions (or modify the example feature-functions) to
## extract useful features for this prediction task.
## 2. Implement an algorithm to learn from the design matrix produced, and to
## make predictions on unseen data. Naive code for these two steps is provided
## below, and marked by TODOs.
##
## Computational Caveat
## --------------------
## Because the biggest of any of the xml files is only around 35MB, the code below
## will parse an entire xml file and store it in memory, compute features, and
## then get rid of it before parsing the next one. Storing the biggest of the files
## in memory should require at most 200MB or so, which should be no problem for
## reasonably modern laptops. If this is too much, however, you can lower the
## memory requirement by using ElementTree.iterparse(), which does parsing in
## a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/
## for an example.

import os
from collections import Counter, deque
try:
    import xml.etree.cElementTree as ET
except ImportError:
    import xml.etree.ElementTree as ET
import numpy as np
from scipy import sparse

import util
import time
import testUtil as test
import classification_methods as methods


def extract_feats(ffs, direc="train", global_feat_dict=None):
    """
    arguments:
      ffs are a list of feature-functions.
      direc is a directory containing xml files (expected to be train or test).
      global_feat_dict is a dictionary mapping feature_names to column-numbers; it
      should only be provided when extracting features from test data, so that
      the columns of the test matrix align correctly.

    returns:
      a sparse design matrix, a dict mapping features to column-numbers,
      a vector of target classes, and a list of system-call-history ids in order
      of their rows in the design matrix.

      Note: the vector of target classes returned will contain the true indices of the
      target classes on the training data, but will contain only -1's on the test
      data
    """
    fds, classes, ids = extract_feats_helper(ffs, direc)

    X,feat_dict = make_design_mat(fds,global_feat_dict)
    return X, feat_dict, np.array(classes), ids

def extract_feats_helper(ffs, direc="train"):
    fds = [] # list of feature dicts
    classes = []
    ids = []
    for datafile in os.listdir(direc):
        # extract id and true class (if available) from filename
        id_str,clazz = datafile.split('.')[:2]
        ids.append(id_str)
        # add target class if this is training data
        try:
            classes.append(util.malware_classes.index(clazz))
        except ValueError:
            # we should only fail to find the label in our list of malware classes
            # if this is test data, which always has an "X" label
            assert clazz == "X"
            classes.append(-1)
        rowfd = {}
        # parse file as an xml document
        tree = ET.parse(os.path.join(direc,datafile))
        # accumulate features
        [rowfd.update(ff(tree)) for ff in ffs]
        fds.append(rowfd)

    return fds, classes, ids

def make_design_mat(fds, global_feat_dict=None):
    """
    arguments:
      fds is a list of feature dicts (one for each row).
      global_feat_dict is a dictionary mapping feature_names to column-numbers; it
      should only be provided when extracting features from test data, so that
      the columns of the test matrix align correctly.

    returns:
        a sparse NxD design matrix, where N == len(fds) and D is the number of
        the union of features defined in any of the fds
    """
    if global_feat_dict is None:
        all_feats = set()
        [all_feats.update(fd.keys()) for fd in fds]
        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])
    else:
        feat_dict = global_feat_dict

    # print "Feature dict:"
    # print "======================================================="
    # print feat_dict
    # print "======================================================="
    # print

    print "Building design matrix..."

    cols = []
    rows = []
    data = []
    for i in xrange(len(fds)):
        temp_cols = []
        temp_data = []
        for feat,val in fds[i].iteritems():
            try:
                # update temp_cols iff update temp_data
                temp_cols.append(feat_dict[feat])
                temp_data.append(val)
            except KeyError as ex:
                if global_feat_dict is not None:
                    pass  # new feature in test data; nbd
                else:
                    raise ex

        # all fd's features in the same row
        k = len(temp_cols)
        cols.extend(temp_cols)
        data.extend(temp_data)
        rows.extend([i]*k)

    assert len(cols) == len(rows) and len(rows) == len(data)


    X = sparse.csr_matrix((np.array(data),
                   (np.array(rows), np.array(cols))),
                   shape=(len(fds), len(feat_dict)))

    print "Done. Shape: " + str(X.shape)
    print

    return X, feat_dict

# -----------------------------------------------------------------------------

## Here are two example feature-functions. They each take an xml.etree.ElementTree object,
# (i.e., the result of parsing an xml file) and returns a dictionary mapping
# feature-names to numeric values.
## TODO: modify these functions, and/or add new ones.
def first_last_system_call_feats(tree):
    """
    arguments:
      tree is an xml.etree.ElementTree object
    returns:
      a dictionary mapping 'first_call-x' to 1 if x was the first system call
      made, and 'last_call-y' to 1 if y was the last system call made.
      (in other words, it returns a dictionary indicating what the first and
      last system calls made by an executable were.)
    """
    c = Counter()
    in_all_section = False
    first = True # is this the first system call
    last_call = None # keep track of last call we've seen
    for el in tree.iter():
        # ignore everything outside the "all_section" element
        if el.tag == "all_section" and not in_all_section:
            in_all_section = True
        elif el.tag == "all_section" and in_all_section:
            in_all_section = False
        elif in_all_section:
            if first:
                c["first_call-"+el.tag] = 1
                first = False
            last_call = el.tag  # update last call seen

    # finally, mark last call seen
    c["last_call-"+last_call] = 1
    return c

def system_call_count_feats(tree):
    """
    arguments:
      tree is an xml.etree.ElementTree object
    returns:
      a dictionary mapping 'num_system_calls' to the number of system_calls
      made by an executable (summed over all processes)
    """
    c = Counter()
    in_all_section = False
    for el in tree.iter():
        # ignore everything outside the "all_section" element
        if el.tag == "all_section" and not in_all_section:
            in_all_section = True
        elif el.tag == "all_section" and in_all_section:
            in_all_section = False
        elif in_all_section:
            c['num_system_calls'] += 1
    return c

def dlls_loaded_feats(tree):
    c = Counter()
    in_all_section = False
    first = True # is this the first system call
    last_call = None # keep track of last call we've seen
    for el in tree.iter():
        # ignore consider only loaded dlls
        if (el.tag == "load_dll" and el.get('filename') != None):
            c[el.get('filename')] += 1
    return c

def registry_keys_feats(tree):
    c = Counter()
    in_all_section = False
    first = True # is this the first system call
    last_call = None # keep track of last call we've seen
    for el in tree.iter():
        # ignore consider only loaded dlls
        if (el.tag == "query_value" and el.get('key') != None):
            c[el.get('key')] += 1
    return c


def system_call_unigram_feats(tree):
    """
    arguments:
      tree is an xml.etree.ElementTree object
    returns:
      a dictionary mapping 'x-count' to the number of calls made to x was the first system call
      (in other words, it returns a dictionary indicating how many times each system call was made)
    """
    c = Counter()
    in_all_section = False
    first = True # is this the first system call
    last_call = None # keep track of last call we've seen
    for el in tree.iter():
        # ignore everything outside the "all_section" element
        if not (el.tag in ["all_section", "thread", "process", "processes"]):
            c[el.tag+'-count'] += 1

    # finally, mark last call seen
    return c

def generate_system_call_ngram_feats(n):    
    def system_call_ngram_feats(tree):
        c = Counter()
        q = deque()
        for el in tree.iter():
            if not (el.tag in ["all_section", "thread", "process", "processes"]):
                q.append(el.tag)
                if len(q) > n:
                    q.popleft()
                    c["-".join(q)] += 1
            else:
                s = "-".join(q)
                if s != "":
                    c[s] += 1
                q.clear()

        return c
    return system_call_ngram_feats

system_call_1gram_feats = generate_system_call_ngram_feats(1)
system_call_2gram_feats = generate_system_call_ngram_feats(2)
system_call_3gram_feats = generate_system_call_ngram_feats(3)
system_call_5gram_feats = generate_system_call_ngram_feats(5)
system_call_8gram_feats = generate_system_call_ngram_feats(8)
system_call_10gram_feats = generate_system_call_ngram_feats(10)
system_call_20gram_feats = generate_system_call_ngram_feats(20)



feature_functions = {
    'first_last_system_call_feats': first_last_system_call_feats,
    'system_call_count_feats': system_call_count_feats,
    'system_call_unigram_feats': system_call_unigram_feats,
    'system_call_1gram_feats': system_call_1gram_feats,
    'system_call_2gram_feats': system_call_2gram_feats,
    'system_call_3gram_feats': system_call_3gram_feats,
    'system_call_5gram_feats': system_call_5gram_feats,
    'system_call_8gram_feats': system_call_8gram_feats,
    'system_call_10gram_feats': system_call_10gram_feats,
    'system_call_20gram_feats': system_call_20gram_feats
}


# -----------------------------------------------------------------------------

## The following function does the feature extraction, learning, and prediction
def main():
    train_dir = "train"
    test_dir = "test"
    outputfile = "mypredictions.csv"  # feel free to change this or take it as an argument

    # TODO put the names of the feature functions you've defined above in this list
    ffs = [first_last_system_call_feats, system_call_count_feats]

    # extract features
    print "extracting training features..."
    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)
    print "done extracting training features"
    print

    # TODO train here, and learn your classification parameters
    print "learning..."
    learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))
    print "done learning"
    print

    # get rid of training data and load test data
    del X_train
    del t_train
    del train_ids
    print "extracting test features..."
    X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)
    print "done extracting test features"
    print

    # TODO make predictions on text data and write them out
    print "making predictions..."
    preds = np.argmax(X_test.dot(learned_W),axis=1)
    print "done making predictions"
    print

    print "writing predictions..."
    util.write_predictions(preds, test_ids, outputfile)
    print "done!"

def mainTest(withhold=0, params={}):
    #default value for params
    params = test.defParams(params)

    train_dir = "train"
    test_dir = "test"

    # TODO put the names of the feature functions you've defined above in this list
    ffs = [system_call_count_feats]
    #ffs = [first_last_system_call_feats, system_call_count_feats]

    # extract features
    print "extracting training features..."
    time1 = time.clock()
    X_train,t_train,train_ids,X_test,y_test,test_ids = test.loadData(params, withhold, ffs)
    time2 = time.clock()
    print "done extracting %d training features, time: %.4f s" % (X_train.shape[1], time2 - time1)
    print

    """
    # TODO train here, and learn your classification parameters
    print "learning..."
    time1 = time.clock()
    learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))
    time2 = time.clock()
    print "done learning"
    print

    # TODO make predictions on text data and write them out
    print "making predictions..."
    preds = np.argmax(X_test.dot(learned_W),axis=1)
    print "done making predictions"
    print
    """
    preds = methods.oneVRest(X_train,t_train,X_test)

    if params['writePredict']==True:
        print "writing predictions..."
        util.write_predictions(preds, test_ids, params['outputFile'])
        print "done!"

# -----------------------------------------------------------------------------


def mainVisualize(params={}):
    withhold = 0

    #default value for params
    params = test.defParams(params)

    train_dir = "train"
    test_dir = "test"

    # TODO put the names of the feature functions you've defined above in this list
    ffs = [system_call_2gram_feats] #[system_call_count_feats]
    #ffs = [first_last_system_call_feats, system_call_count_feats]

    # extract features
    print "extracting training features..."
    time1 = time.clock()
    X_train,t_train,train_ids,X_test,t_test,test_ids = test.loadData(params, withhold, ffs)
    time2 = time.clock()
    print "done extracting %d training features, time: %.4f s" % (X_train.shape[1], time2 - time1)
    print    

    import matplotlib.pyplot as plt
    import matplotlib.cm as cm

    plt.matshow(X_train.T.toarray(),cmap=cm.get_cmap('Reds'))
    plt.colorbar()
    plt.show()

    # plt.spy(X_train)
    # plt.show()

def mainVisualizeFeatures(params={}):
    withhold = 0

    #default value for params
    params = test.defParams(params)

    train_dir = "train"
    test_dir = "test"

    # TODO put the names of the feature functions you've defined above in this list
    ffs = [system_call_2gram_feats] #[system_call_count_feats]
    #ffs = [first_last_system_call_feats, system_call_count_feats]

    # extract features
    print "extracting training features..."
    time1 = time.clock()
    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)
    time2 = time.clock()
    print "done extracting %d training features, time: %.4f s" % (X_train.shape[1], time2 - time1)
    print    

    feature_data = np.zeros((len(global_feat_dict),2))
    feature_names = []

    for (feature, index) in global_feat_dict.iteritems():
        feature_data[index][0] = X_train[:,index].mean()
        feature_data[index][1] = X_train[:,index].toarray().std()
        feature_names.append(feature)

    import matplotlib.pyplot as plt
    import matplotlib.cm as cm

    ind = np.arange(len(global_feat_dict))
    feature_data.sort(axis=0)

    plt.bar(ind, feature_data[:,0], yerr=feature_data[:,1])
    plt.xticks(ind,feature_names,rotation='vertical')

    plt.show()

# -----------------------------------------------------------------------------

def testCatAcc(preds, y_test):
    return float(np.sum(preds == y_test)) / len(y_test)

def testCatErr(preds, y_test):
    return 1 - testCatAcc(preds, y_test)

def mainTestIter(withhold=0, params=None):
    from sklearn import cross_validation
    import classification_methods as classif

    #default value for params
    if params==None:
        params = {}

    params = dict({'withhold': 0,
      'load': None,
      'extractFile': None,
      'loadTest': False,

      # arguments to `learn`
      'options': {},

      # the option to cycle through
      'option': None,

      # range of values to cycle through
      'range': [],

      # k-fold cross-validation
      'n_folds': 10,

      # names of feature functions to use
      'ffs': ['system_call_unigram_feats'] 
    }, **params)

    train_dir = "train"
    test_dir = "test"

    # TODO put the names of the feature functions you've defined above in this list
    ffs = [feature_functions[f] for f in params['ffs']]

    print
    print "extracting training/testing features..."
    time1 = time.clock()
    # X_train, y_train, train_ids, X_test, y_test, test_ids = test.loadData(params, withhold, ffs)
    X, y, ids, _, _, _ = test.loadData(params, withhold, ffs)
    time2 = time.clock()
    print "done extracting training/testing features", time2-time1, "s"
    print "%d data, %d features" % X.shape
    print



    # options for the learning engine
    options = params['options']

    # array to store errors for various values of learning options
    errors = []

    # iterate through each value of `params['option']` in `params['range']`
    # and calculate the error for that value
    print "iterating over values of %s from %s ... %s" % (params['option'], params['range'][0], params['range'][-1])
    print "================================================================================"
    for (i, value) in enumerate(params['range']):
        print "%s = %s" % (params['option'], str(value))
        op = dict(options)
        op[params['option']] = value

        # generate k cross-validation folds
        kf = cross_validation.KFold(len(y),n_folds=params['n_folds'],shuffle=True)
        print "k-fold cross-validation with %d folds" % params['n_folds']
        cv_err = []

        # for each cv fold
        for train,tests in kf:

            # generate partition
            X_train, y_train, X_test, y_test = X[train], y[train], X[tests], y[tests]

            # train and predict
            print "learning and predicting..."
            time1 = time.clock()
            preds = classif.classify(X_train,y_train,X_test, **op)
            time2 = time.clock()
            print "done learning and predicting, ", time2-time1, "s"
            print

            # cross-validate
            cv_err.append(testCatErr(preds, y_test))
            print "Err on withheld data: %f" % cv_err[-1]
            print

        # calculate mean, std. across folds
        cv_err_mean, cv_err_std = np.mean(cv_err), np.std(cv_err)

        print
        print "Avg. Err: %f" % cv_err_mean
        print "Std. Err: %f" % cv_err_std
        errors.append((cv_err_mean, cv_err_std))

        print "--------------------------------------------------------------------------------"

    print "================================================================================"

    # tabulate results
    results = dict()
    
    print "Features:"
    print params['ffs']
    print 
    print "Options:"
    print options
    print

    print "Results:"
    print "%18s \t Err \t std" % params['option']
    for (i, value) in enumerate(params['range']):
        print "%18s \t %f \t %f" % (value, errors[i][0], errors[i][1])
        if(isinstance(value, list)):
            value = tuple(value)

        results[value] = errors[i]

    return results


def mainTestPred(withhold=0, params=None):
    from sklearn import cross_validation
    import classification_methods as classif

    #default value for params
    if params==None:
        params = {}

    params = dict({'withhold': 0,
      'load': None,
      'extractFile': None,

      # arguments to `learn`
      'options': {},

      # k-fold cross-validation
      'n_folds': 10,

      # feature functions
      'ffs': ['system_call_unigram_feats'] 

    }, **params)

    op = dict(params['options'])


    train_dir = "train"
    test_dir = "test"
    outputfile = "mypredictions.csv"  # feel free to change this or take it as an argument

    # TODO put the names of the feature functions you've defined above in this list
    ffs = [feature_functions[f] for f in params['ffs']]

    # extract features
    print "extracting training features..."
    X_train,global_feat_dict,y_train,train_ids = extract_feats(ffs, train_dir)
    print "done extracting training features"
    print

    print "extracting test features..."
    X_test,_,y_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)
    print "done extracting test features"
    print

    # TODO make predictions on text data and write them out
    print "making predictions..."
    preds = classif.classify(X_train,y_train,X_test, **op)
    print "done making predictions"
    print

    print "writing predictions..."
    util.write_predictions(preds, test_ids, params['outputFile'])
    print "done!"



if __name__ == "__main__":
    main()
