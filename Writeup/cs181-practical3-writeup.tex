\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{commath}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Declare commands
\newcommand{\mat}[1]{\mathbf{#1}}

\title{CS 181 -- Practical 3}
\author{Casey Grun, Sam Kim, Rhed Shi}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

% -----------------------------------------------------------------------------
\section{Warmup}


% -----------------------------------------------------------------------------
\section{Classification}

Our challenge was to classify a set of programs, based on traces of their system calls, as either a type of malware or a non-malicious program. Given an $N \times D$ matrix $\mat{X}$ representing the programs, along with a $N$-dimensional vector $\vec{t}$ of training labels, produce a function 
$$y : \mat{X'} \mapsto \vec{t'}$$.
That is, a function which could produce labels $\vec{t'}$ for some matrix $\mat{X'}$. There were two parts to this challenge: determining the \emph{feature functions} to generate rows of the matrix $\mat{X}$ based on the system calls for each malware program, and determining what classification algorithm to use to generate the function $y$.

\subsection{Feature Functions}

We evaluated a number of feature functions:
\begin{description}
  \item[System call counts]
  \item[$n$-grams]
  \item[DLLs loaded]
  \item[Registry keys]
\end{description}

\subsection{Classification Methods}
We evaluated a number of classification learning algorithms, as implemented by the scikit-learn Python library. These include:
\begin{description}
  \item[One-vs-rest] As discussed in the textbook, this algorithm fits one classifier per class to classify if a point is in a class $C_k$ or not. We used linear discriminant functions.
  \item[One-Vs-One] This algorithm introduces a discriminant function for every pair of classes, where each point is classified according to a majority of decisions among the discriminant functions. We used linear discriminant functions.
  \item[Logistic regression] This algorithm minimizes an error based on the training set to optimize the parameters for the likelihood function.
  \item[Decision trees] This algorithm goes down a tree of binary nodes in which the inputs are used to determine which path to go down, which ultimately assign probabilities to the target variable.
  \item[Forests of randomized trees] Related to the decision trees, this algorithm uses a set of trees, where each tree is built using random decisions. Whereas a decision tree will use the best split among all the features for each node, the random tree will pick the best split from a random subset of features. The trees are built independently and then their results are averaged to make predictions. This results in the bias increasing (due to not picking the best split) but the variance decreasing (due to averaging) which offsets the increase in bias.
  \item[Forests of extremely randomized trees] This is similar to the forests of randomized trees algorithm, except that splits are computed slightly differently in building the random trees. In addition to using a random subset of features for each node, instead of picking the best feature, thresholds are drawn randomly for each feature and these thresholds are picked as the splitting rules. This results in an even higher bias but lower variance.
\end{description}

We evaluated these learning algorithms based on the extracted features of system call counts and system call bigrams, and using cross-validation methods to withhold 10\% of the data over 10 trials and averaging the percent errors over the predictions on the withheld data. Standard deviations over the 10 trials were calculated and the average running time for a single learning and prediction run are also noted. The results are summarized in the table below:\\

\begin{tabular}{l | r | l | l}
Classification algorithm & \% error & Standard deviation & Running time (s)\\
\hline
One-vs-rest (linear) & 12.7 & 2.4 & 7-8 \\
One-vs-one (linear) & 12.9 & 1.4 & 7-9\\
Logistic regression & 11.5 & 1.5 & 600-700\\
Decision tree & 10.9 & 1.3 & 50-60 \\
Random trees & 10.4 & 1.2 & 8-10\\
Extremely random trees & 9.7 & 1.6 &16-20\\
\end{tabular}
\\
\\
Additionally, on Kaggle, the one-vs-rest, decision tree, and extremely random trees classification algorithms using the system call counts and system call bigrams reached accuracies of 68.5\%, 79.1\%, and 81.0\%, respectively.\\


% -----------------------------------------------------------------------------
\section{Conclusion}


% -----------------------------------------------------------------------------
\begingroup
\begin{thebibliography}{9}

%\bibitem{LSMR}
%Fong, David Chin-Lung, and Michael Saunders. "LSMR: An iterative algorithm for sparse least-squares problems."
%\emph{SIAM Journal on Scientific Computing} 33.5 (2011): 2950-2971

\end{thebibliography}
\endgroup

\end{document}